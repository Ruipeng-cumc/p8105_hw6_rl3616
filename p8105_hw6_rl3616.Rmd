---
title: "p8105_hw5_rl3616"
author: "Ruipeng Li"
date: "`r format(Sys.Date())`"
output: github_document
---
```{r}
library(tidyverse)
library(broom)
library(modelr)
```
## Problem 1

```{r data clean}
homicides <- read.csv("data/homicide-data.csv")

homicides_df <- homicides |> 
  janitor::clean_names() |>
  mutate(
    city_state = str_c(city, ", ", state),
    resolved = ifelse(disposition == "Closed by arrest", 1, 0),
    victim_age = str_trim(victim_age),
    victim_age = na_if(victim_age, "Unknown"),
    victim_age = parse_number(victim_age)
  ) |> 
  filter(
    !city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL"),
    victim_race %in% c("White", "Black")
    )

```

```{r for baltimore}
baltimore_mod <- homicides_df |>
  filter(city_state == "Baltimore, MD")

baltimore_mod <- glm(
  resolved ~ victim_age + victim_sex + victim_race,
  data = homicides_df,
  family = binomial
)

broom::tidy(baltimore_mod, exponentiate = TRUE, conf.int = TRUE) |> 
  filter(term == "victim_sexMale")
```

```{r for all city}
city_results <- homicides_df |>
  group_by(city_state) |>
  nest() |>
  mutate(
    model = map(data, ~ glm(
      resolved ~ victim_age + victim_sex + victim_race,
      data = .x,
      family = binomial
    )),
    tidy = map(model, ~ tidy(.x, exponentiate = TRUE, conf.int = TRUE))
  ) |>   #Seems I got correct result, but an unknown warning occurred.
  unnest(tidy) |>
  filter(term == "victim_sexMale") |> 
  select(-data, -model)
```  

```{r estimated ORs and CIs for each city, fig.width=8, fig.height=8, warning=FALSE}
city_results |> 
  ggplot(aes(
    x = fct_reorder(city_state, estimate),
    y = estimate
  )) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2) +
  coord_flip() +
  labs(
    x = "City",
    y = "Adjusted OR (Male vs Female)",
    title = "Adjusted Odds Ratios of Solved Homicides by City"
  )
```

According to the chart, most cities have odds ratios less than 1 for the “male vs female” contrast. Since female victims serve as the reference category in the model, an OR < 1 indicates that homicide cases involving male victims are less likely to be solved. In other words, cases with female victims tend to have higher odds of being solved in most cities.

## Problem 2
```{r}
library(p8105.datasets)
data("weather_df")

set.seed(123)

weather_boot <- weather_df |> 
  bootstrap(n = 5000, id = "strap_id")
```

```{r estimate}
boot_results <- weather_boot |>
  mutate(
    model  = map(strap, ~ lm(tmax ~ tmin + prcp, data = .x)),
    glnc   = map(model, glance),   # get R^2
    tidy   = map(model, tidy)      # get betas
  ) |>
  mutate(
    r_sq = map_dbl(glnc, "r.squared"),
    beta_ratio = map_dbl(tidy, ~ {
      b1 = .x |> filter(term == "tmin") |> pull(estimate)
      b2 = .x |> filter(term == "prcp") |> pull(estimate)
      b1 / b2
    })
  ) |>
  select(strap_id, r_sq, beta_ratio)

```

```{r}
boot_results |>
  pivot_longer(r_sq:beta_ratio,
               names_to = "param",
               values_to = "estimate") |>
  ggplot(aes(x = estimate)) +
  geom_histogram() +
  facet_wrap(~ param, scales = "free_x") +
  labs(x = "Estimate", y = "Count",
       title = "Boostrap distribution of R^2 amd beta1/beta2")
  
         
         
```    

Based on the plots, both distributions exhibit varying degrees of left skew.
The skewness is more pronounced for the beta ratio, with a peak around 180 and a mean that lies to the left of the mode.
In contrast, the distribution of R^2 is closer to normal, showing only slight left skew, with a peak between 0.94 and 0.942 and a mean that is also positioned to the left of the peak.

```{r}
ci_results <- boot_results |>
  pivot_longer(r_sq:beta_ratio,
               names_to = "param",
               values_to = "estimate") |> 
  group_by(param) |> 
  summarise(
    ci_low = quantile(estimate, 0.025),
    ci_high = quantile(estimate, 0.975)
  )

ci_results
```   

As result, 95% confidence interval for $\hat{r}^2$ are (`r ci_results$ci_low[2]`, `r ci_results$ci_high[2]`);
95% confidence interval for $\frac{\hat{\beta_1}}{\hat{\beta_2}}$ are (`r ci_results$ci_low[1]`, `r ci_results$ci_high[1]`)


## Problem 3
```{r cleaning}
birthweight_df <- read.csv("data/birthweight.csv") |> 
  janitor::clean_names() |> 
  mutate(
    babysex = ifelse(babysex == 1, "male", "female"),
    frace = recode(frace,
                   `1` = "White",
                   `2` = "Black",
                   `3` = "Asian",
                   `4` = "Puerto Rican", 
                   `8` = "Other", 
                   `9` = "Unknown"),
    mrace = recode(mrace,
                   `1` = "White",
                   `2` = "Black",
                   `3` = "Asian",
                   `4` = "Puerto Rican", 
                   `8` = "Other"),
    malform = ifelse(malform == 0, "absent", "present")
  ) |> 
  rename(
    baby_sex               = babysex,
    baby_head_circ        = bhead,
    baby_length           = blength,
    baby_weight           = bwt,
    
    mother_weight_delivery = delwt,
    family_income          = fincome,
    father_race            = frace,
    
    gestational_age        = gaweeks,
    malformation           = malform,
    
    menarche_age           = menarche,
    mother_height          = mheight,
    mother_age_delivery    = momage,
    
    mother_race            = mrace,
    parity                 = parity,
    
    prev_lowbirth_count    = pnumlbw,
    prev_small_gest_age    = pnumsga,
    
    pre_pregnancy_bmi      = ppbmi,
    mother_pre_preg_weight = ppwt,
    
    cigarettes_per_day     = smoken,
    weight_gain_pregnancy  = wtgain
  )
```

# Main model  
To construct my main model, I used a data-driven approach.
I started by fitting a large model that included all variables that might reasonably affect an infant’s birthweight. This allowed me to see which predictors were truly meaningful and which ones contributed very little.

Based on the full model, I examined significance levels, effect sizes, and practical relevance. Variables that appeared unimportant or had minimal influence were removed to avoid overfitting and to make the model easier to interpret.

After selecting the predictors that showed clearer and more consistent effects, I built a simplified and more interpretable main model. This refined model strikes a balance between predictive performance and interpretability, and it serves as the foundation for comparing the other two models in the analysis.
```{r Basic model}
bw_formula_main <- 
  baby_weight ~ 
    gestational_age +
    baby_length +
    baby_head_circ +
    mother_height +
    pre_pregnancy_bmi +
    weight_gain_pregnancy +
    cigarettes_per_day +
    baby_sex

bw_model_main <- lm(bw_formula_main, data = birthweight_df)
summary(bw_model_main)

```

```{r Basic model plot}
birthweight_df |> 
  add_predictions(bw_model_main) |> 
  add_residuals(bw_model_main) |> 
  ggplot(aes(x = pred, y = resid)) +
  geom_point(alpha = 0.2) +
  geom_hline(yintercept = 0, color = "red") +
  labs(
    title = "Residuals vs Fitted - Main Model",
    x = "Predicted Birthweight",
    y = "Residuals"
  )
```

The residuals in the graph are mostly randomly scattered, without any issues of arcing or diffusion.   
The point cloud is distributed around 0, and there is no systematic bias, which indicates that my model fits very stably and there is no obvious heteroscedasticity problem.

```{r small model}
bw_formula_small <- baby_weight ~ baby_length + gestational_age

bw_model_small <- lm(bw_formula_small, data = birthweight_df)
summary(bw_model_small)
```

```{r big model}
bw_formula_big <- baby_weight ~ baby_head_circ * baby_length * baby_sex

bw_model_big <- lm(bw_formula_big, data = birthweight_df)
summary(bw_model_big)
```

To evaluate predictive performance, we used Monte Carlo cross-validation with 10 random train–test splits generated by `crossv_mc.` For each split, the models were fit on the training set and tested on the held-out data using `purrr::map2_dbl` to compute RMSE.

```{r}
cv_bw_df <- crossv_mc(birthweight_df, 10)

cv_bw_df <- cv_bw_df |> mutate(
  # Fit the 3 models
  fit_main  = map(train, ~ lm(bw_formula_main, 
                              data = .x)),
  
  fit_small = map(train, ~ lm(bw_formula_small,
                              data = .x)),
  
  fit_big   = map(train, ~ lm(bw_formula_big,
                              data = .x)),
  
  # Compute RMSE
  rmse_main  = map2_dbl(fit_main,  test, ~ sqrt(mean((predict(.x, newdata = as.data.frame(.y)) - as.data.frame(.y)$baby_weight)^2))),
  rmse_small = map2_dbl(fit_small, test, ~ sqrt(mean((predict(.x, newdata = as.data.frame(.y)) - as.data.frame(.y)$baby_weight)^2))),
  rmse_big   = map2_dbl(fit_big,   test, ~ sqrt(mean((predict(.x, newdata = as.data.frame(.y)) - as.data.frame(.y)$baby_weight)^2)))
)

cv_bw_df |> summarise(
  main_rmse  = mean(rmse_main),
  small_rmse = mean(rmse_small),
  big_rmse   = mean(rmse_big)
)
```

As result, our proposed main model achieved the lowest average RMSE (≈285 g), indicating the best out-of-sample predictive performance among the three models.

The small model had substantially higher prediction error (≈341 g), while the interaction model performed slightly worse than the main model (≈294 g).